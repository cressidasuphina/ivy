{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7h4I0ehaWTxvo2kBjEsgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cressidasuphina/ivy/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E9e4_IeqBDtN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,  weight, bias, stride=1, padding=0):\n",
        "        super(CustomConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "\n",
        "        self.weight = weight #torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
        "        self.bias = bias #torch.zeros(out_channels)\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, in_channels, in_height, in_width = input.size()\n",
        "        out_height = (in_height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "        out_width = (in_width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "\n",
        "        output = torch.zeros(batch_size, self.out_channels, out_height, out_width)\n",
        "        # Iterating over each element in the batch, output channel, and spatial dimensions\n",
        "        for b in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h_out in range(out_height):\n",
        "                    for w_out in range(out_width):\n",
        "                      # Computing the input slice to apply convolution\n",
        "                        h_start = h_out * self.stride\n",
        "                        w_start = w_out * self.stride\n",
        "                        h_end = h_start + self.kernel_size\n",
        "                        w_end = w_start + self.kernel_size\n",
        "\n",
        "                        input_slice = input[b, :, h_start:h_end, w_start:w_end]\n",
        "                        # Computing the output value using convolution and bias\n",
        "\n",
        "                        output[b, c_out, h_out, w_out] = torch.sum(input_slice * self.weight[c_out]) + self.bias[c_out]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        batch_size, in_channels, in_height, in_width = input.size()\n",
        "        out_channels, _, kernel_height, kernel_width = self.weight.size()\n",
        "        _, _, out_height, out_width = grad_output.size()\n",
        "\n",
        "        grad_input = torch.zeros_like(input)\n",
        "        grad_weight = torch.zeros_like(self.weight)\n",
        "        grad_bias = torch.zeros_like(self.bias)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c_out in range(out_channels):\n",
        "                for h_out in range(out_height):\n",
        "                    for w_out in range(out_width):\n",
        "                        # Computing the input slice corresponding to the current gradient\n",
        "                        h_start = h_out * self.stride\n",
        "                        h_start = h_out * self.stride\n",
        "                        w_start = w_out * self.stride\n",
        "                        h_end = h_start + kernel_height\n",
        "                        w_end = w_start + kernel_width\n",
        "\n",
        "                        input_slice = input[b, :, h_start:h_end, w_start:w_end]\n",
        "                        # Updating gradients using chain rule and derivatives\n",
        "\n",
        "                        grad_input[b, :, h_start:h_end, w_start:w_end] += self.weight[c_out] * grad_output[b, c_out, h_out, w_out]\n",
        "                        grad_weight[c_out] += input_slice * grad_output[b, c_out, h_out, w_out]\n",
        "                        grad_bias[c_out] += grad_output[b, c_out, h_out, w_out]\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias"
      ],
      "metadata": {
        "id": "oxSdtmTZB4J2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "input_channels = 3\n",
        "output_channels = 8\n",
        "input_height = 28\n",
        "input_width = 28\n",
        "kernel_size = 3\n",
        "output_height = input_height - kernel_size + 1\n",
        "output_width = input_width - kernel_size + 1\n",
        "\n",
        "\n",
        "grad_output = torch.randn(batch_size, output_channels, output_height, output_width)\n",
        "input_data = torch.randn(batch_size, input_channels, input_height, input_width)\n",
        "input_data.requires_grad = True\n",
        "# PyTorch conv2d operation using the same inputs\n",
        "pytorch_conv2d = torch.nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size)\n",
        "pytorch_output = pytorch_conv2d(input_data)\n",
        "\n",
        "weights = pytorch_conv2d.weight\n",
        "bias = pytorch_conv2d.bias\n",
        "\n",
        "# Computing the gradient of the loss with respect to the weights using PyTorch's autograd\n",
        "pytorch_output.backward(grad_output)\n",
        "pytorch_grad_weight = pytorch_conv2d.weight.grad\n",
        "pytorch_grad_input = input_data.grad\n",
        "pytorch_grad_bias = pytorch_conv2d.bias.grad"
      ],
      "metadata": {
        "id": "AgbprOADCOMK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_conv2d = CustomConv2d(in_channels=1, out_channels=8, kernel_size=3, weight=weights, bias=bias ,stride=1, padding=0)"
      ],
      "metadata": {
        "id": "fLIZDoUvCvy0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_output = custom_conv2d.forward(input_data)\n",
        "custom_output[0,0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgiZwmp2DFx7",
        "outputId": "806d1f30-7e35-426e-d936-3d7274bcd8f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1085,  0.1187, -0.8726, -1.0744,  0.5482, -0.4275, -0.1119,  0.1941,\n",
              "        -0.1385,  0.6839, -0.2433,  0.9042, -0.0550,  0.8143, -0.5339, -0.1639,\n",
              "        -0.1327, -0.5068, -0.7393, -0.1712, -0.1252,  0.3437,  0.1382, -0.0077,\n",
              "        -0.1278, -0.2230], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_output[0,0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taYq9uV6DTTD",
        "outputId": "6eead38f-40bd-4264-ffce-86098221f76c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1085,  0.1187, -0.8726, -1.0744,  0.5482, -0.4275, -0.1119,  0.1941,\n",
              "        -0.1385,  0.6839, -0.2433,  0.9042, -0.0550,  0.8143, -0.5339, -0.1639,\n",
              "        -0.1327, -0.5068, -0.7393, -0.1712, -0.1252,  0.3437,  0.1382, -0.0077,\n",
              "        -0.1278, -0.2230], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch Conv2d forward output and my custom conv2d comparison\n",
        "mse_weights = ((pytorch_output - custom_output) ** 2).mean().item()\n",
        "mse_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wPde3PjEfci",
        "outputId": "885a57ee-7a50-4ca9-ba75-e963d81e5e3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.831173116859886e-15"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting my custom Conv2d grad_input, grad_weight, grad_bias and its comparisons\n",
        "grad_input, grad_weight, grad_bias = custom_conv2d.backward(input_data, grad_output)"
      ],
      "metadata": {
        "id": "T0a75L9vDi4R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_input = ((pytorch_grad_input - grad_input) ** 2).mean().item()\n",
        "mse_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3evtIOEqDq_q",
        "outputId": "fdcbeab8-90a9-4cc0-c60b-6448cb5ec608"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.600986273419443e-14"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_bias = ((pytorch_grad_bias - grad_bias) ** 2).mean().item()\n",
        "mse_bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDCj8PacD92K",
        "outputId": "16733782-5149-4aee-edbf-8b6dadebfbfb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1181100489920937e-10"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_weights = ((pytorch_grad_weight - grad_weight) ** 2).mean().item()\n",
        "mse_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjMvweR2ESoh",
        "outputId": "5c262239-d668-4b74-8904-b355b001ab9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9024425090830022e-10"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_transpose2d_basic(input, weight, bias, stride=1, padding=0, output_padding=0, dilation=1):\n",
        "\n",
        "    batch_size, in_channels, in_height, in_width = input.size()\n",
        "    _, out_channels, kernel_height, kernel_width = weight.size()\n",
        "\n",
        "\n",
        "    # Calculating output dimensions\n",
        "    out_height = (in_height - 1 + 2 * padding) * stride - 2 * padding + dilation * (kernel_height - 1) + output_padding + 1\n",
        "    out_width = (in_width - 1 + 2 * padding) * stride - 2 * padding + dilation * (kernel_width - 1) + output_padding + 1\n",
        "\n",
        "\n",
        "\n",
        "    output = torch.zeros(batch_size, out_channels, out_height, out_width)\n",
        "\n",
        "    # Iterating over each batch and channel\n",
        "    for b in range(batch_size):\n",
        "        for c_out in range(out_channels):\n",
        "            for h_out in range(out_height):\n",
        "                for w_out in range(out_width):\n",
        "                    # Computing the receptive field in the input\n",
        "                    h_start = h_out * stride - padding + dilation * (kernel_height - 1)\n",
        "                    w_start = w_out * stride - padding + dilation * (kernel_width - 1)\n",
        "                    h_end = h_start + dilation * (kernel_height - 1) + 1\n",
        "                    w_end = w_start + dilation * (kernel_width - 1) + 1\n",
        "\n",
        "                    if h_start < 0 or w_start < 0 or h_end > in_height or w_end > in_width:\n",
        "                        continue\n",
        "\n",
        "                    # Computing the sum of products for this output location\n",
        "                    for c_in in range(in_channels):\n",
        "                        output[b, c_out, h_out, w_out] += torch.sum(\n",
        "                            input[b, c_in, h_start:h_end:dilation, w_start:w_end:dilation] * weight[c_in]\n",
        "                        )\n",
        "\n",
        "                    # Adding bias if provided\n",
        "                    if bias is not None:\n",
        "                        output[b, c_out, h_out, w_out] += bias[c_out]\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example usage\n",
        "batch_size = 1\n",
        "input_channels = 3\n",
        "output_channels = 8\n",
        "kernel_size = 3\n",
        "input_height = 28\n",
        "input_width = 28\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_data = torch.randn(batch_size, input_channels, input_height, input_width)\n",
        "\n",
        "input_data.requires_grad = True\n",
        "\n",
        "#PyTorch ConvTranspose2d operation\n",
        "#torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n",
        "pytorch_conv_transpose2d = nn.ConvTranspose2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size)\n",
        "pytorch_output = pytorch_conv_transpose2d(input_data)\n",
        "\n",
        "\n",
        "weights = pytorch_conv_transpose2d.weight\n",
        "bias = pytorch_conv_transpose2d.bias\n",
        "\n",
        "# Custom ConvTranspose2d operation\n",
        "output = conv_transpose2d_basic(input_data, weights, bias)\n",
        "#My custom ConvTranspose2d forward operation and Pytorch ConvTranspose2d operation comparison\n",
        "mse = ((output - pytorch_output) ** 2).mean().item()\n",
        "print(\"MSE:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opapq3Ukodop",
        "outputId": "69ca3c89-f649-485e-b915-16d7bd2d8047"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.44191673398017883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_transpose2d_basic_backward(output_grad, input, weight, bias, stride=1, padding=0, output_padding=0, dilation=1):\n",
        "    batch_size, out_channels, out_height, out_width = output_grad.size()\n",
        "    in_channels, _, kernel_height, kernel_width = weight.size()\n",
        "\n",
        "    grad_input = torch.zeros_like(input)\n",
        "    grad_weight = torch.zeros_like(weight)\n",
        "    grad_bias = torch.zeros_like(bias) if bias is not None else None\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        for c_out in range(out_channels):\n",
        "            for h_out in range(out_height):\n",
        "                for w_out in range(out_width):\n",
        "                    h_start = h_out * stride - padding + dilation * (kernel_height - 1)\n",
        "                    w_start = w_out * stride - padding + dilation * (kernel_width - 1)\n",
        "                    h_end = h_start + dilation * (kernel_height - 1) + 1\n",
        "                    w_end = w_start + dilation * (kernel_width - 1) + 1\n",
        "                    #checking if the computed receptive field is within valid bounds to prevent indexing errors, if the condition is not met, the continue statement skips thatt iteration of the loop\n",
        "                    if h_start < 0 or w_start < 0 or h_end > input.size(2) or w_end > input.size(3):\n",
        "                        continue\n",
        "                    #Calculating gradients for input, weight, and bias\n",
        "                    for c_in in range(in_channels):\n",
        "                        grad_input[b, c_in, h_start:h_end:dilation, w_start:w_end:dilation] += (\n",
        "                            output_grad[b, c_out, h_out, w_out] * weight[c_in, c_out]\n",
        "                        )\n",
        "                        grad_weight[c_in, c_out] += (\n",
        "                            output_grad[b, c_out, h_out, w_out]\n",
        "                            * input[b, c_in, h_start:h_end:dilation, w_start:w_end:dilation]\n",
        "                        )\n",
        "\n",
        "                    if grad_bias is not None:\n",
        "                        grad_bias[c_out] += output_grad[b, c_out, h_out, w_out]\n",
        "\n",
        "    return grad_input, grad_weight, grad_bias\n",
        "\n",
        "output_height = 30 #input_height - kernel_size + 1\n",
        "output_width = 30 #input_width - kernel_size + 1\n",
        "\n",
        "grad_output = torch.randn(batch_size, output_channels, output_height, output_width)\n",
        "\n",
        "# Compute the gradient of the loss with respect to the weights using PyTorch's autograd\n",
        "pytorch_output.backward(grad_output)\n",
        "pytorch_grad_weight = pytorch_conv_transpose2d.weight.grad\n",
        "pytorch_grad_input = input_data.grad\n",
        "pytorch_grad_bias = pytorch_conv_transpose2d.bias.grad\n",
        "\n",
        "\n",
        "grad_input, grad_weight, grad_bias = conv_transpose2d_basic_backward(grad_output, input_data, weights, bias )\n",
        "\n",
        "# Calculating mse between PyTorch gradients and custom gradients\n",
        "mse_input = ((pytorch_grad_input - grad_input) ** 2).mean().item()\n",
        "mse_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3mDo6fLByO_",
        "outputId": "f79d4487-bf45-460e-8cd4-dea5580eb894"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5641183257102966"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    }
  ]
}